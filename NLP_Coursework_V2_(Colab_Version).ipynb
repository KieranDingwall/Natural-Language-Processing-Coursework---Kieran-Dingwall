{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KieranDingwall/Natural-Language-Processing-Coursework---Kieran-Dingwall/blob/main/NLP_Coursework_V2_(Colab_Version).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47b40ace",
      "metadata": {
        "id": "47b40ace"
      },
      "source": [
        "# Natural Language Processing Coursework"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f5dba9e",
      "metadata": {
        "id": "8f5dba9e"
      },
      "source": [
        "By Kieran Dingwall - Student Number: 2208619"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1528fb70",
      "metadata": {
        "id": "1528fb70"
      },
      "source": [
        "### Section 1 - Dataset Exploring and Pre Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: https://archive.ics.uci.edu/dataset/837/product+classification+and+clustering"
      ],
      "metadata": {
        "id": "rOE6kfWi_PR-"
      },
      "id": "rOE6kfWi_PR-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For my classification model, I will be using the ‘Product Classification and Clustering’ dataset that can be found on ‘UC Irvine Machine Learning Repository’. This dataset was taken from pricerunner, which is a popular product comparison platform. The dataset includes 35311 products in 10 different categories from 306 different merchants. The aim is to train the model to recognise different products and predict what product category they belong too."
      ],
      "metadata": {
        "id": "8CsLVUbC-0G7"
      },
      "id": "8CsLVUbC-0G7"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1dfaab51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dfaab51",
        "outputId": "401f3901-152d-4ab1-b17a-d0c1ba30938b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "       Product ID                                      Product Title  \\\n",
            "0               1                    apple iphone 8 plus 64gb silver   \n",
            "1               2                apple iphone 8 plus 64 gb spacegrau   \n",
            "2               3  apple mq8n2b/a iphone 8 plus 64gb 5.5 12mp sim...   \n",
            "3               4                apple iphone 8 plus 64gb space grey   \n",
            "4               5  apple iphone 8 plus gold 5.5 64gb 4g unlocked ...   \n",
            "...           ...                                                ...   \n",
            "35306       47350  smeg fab28 60cm retro style right hand hinge f...   \n",
            "35307       47351  smeg fab28 60cm retro style left hand hinge fr...   \n",
            "35308       47352  smeg fab28 60cm retro style left hand hinge fr...   \n",
            "35309       47355     candy 60cm built under larder fridge cru160nek   \n",
            "35310       47358           neff k4316x7gb built under larder fridge   \n",
            "\n",
            "        Merchant ID   Cluster ID             Cluster Label   Category ID  \\\n",
            "0                 1            1  Apple iPhone 8 Plus 64GB          2612   \n",
            "1                 2            1  Apple iPhone 8 Plus 64GB          2612   \n",
            "2                 3            1  Apple iPhone 8 Plus 64GB          2612   \n",
            "3                 4            1  Apple iPhone 8 Plus 64GB          2612   \n",
            "4                 5            1  Apple iPhone 8 Plus 64GB          2612   \n",
            "...             ...          ...                       ...           ...   \n",
            "35306            59        47517          Smeg FAB28 Cream          2623   \n",
            "35307            59        47518            Smeg FAB28 Red          2623   \n",
            "35308            59        47519           Smeg FAB28 Pink          2623   \n",
            "35309           125        47524             Candy CRU16.0          2623   \n",
            "35310           179        47525                Neff K4316          2623   \n",
            "\n",
            "       Category Label  \n",
            "0       Mobile Phones  \n",
            "1       Mobile Phones  \n",
            "2       Mobile Phones  \n",
            "3       Mobile Phones  \n",
            "4       Mobile Phones  \n",
            "...               ...  \n",
            "35306         Fridges  \n",
            "35307         Fridges  \n",
            "35308         Fridges  \n",
            "35309         Fridges  \n",
            "35310         Fridges  \n",
            "\n",
            "[35311 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load Dataset\n",
        "from google.colab import files, drive #import colab functionality\n",
        "drive.mount(\"/content/drive\")\n",
        "# Path to where mine is - This will need changed\n",
        "path = '/content/drive/MyDrive/Uni/4th Year 1st Sem/Natural Language Processing/NLP Coursework/pricerunner_aggregate.csv'\n",
        "product_data = pd.read_csv(path)\n",
        "print(product_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4d101a63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d101a63",
        "outputId": "56dc0ab6-7331-48e9-da1d-350fd2e0311d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           Product Title  \\\n",
            "0                        apple iphone 8 plus 64gb silver   \n",
            "1                    apple iphone 8 plus 64 gb spacegrau   \n",
            "2      apple mq8n2b/a iphone 8 plus 64gb 5.5 12mp sim...   \n",
            "3                    apple iphone 8 plus 64gb space grey   \n",
            "4      apple iphone 8 plus gold 5.5 64gb 4g unlocked ...   \n",
            "...                                                  ...   \n",
            "35306  smeg fab28 60cm retro style right hand hinge f...   \n",
            "35307  smeg fab28 60cm retro style left hand hinge fr...   \n",
            "35308  smeg fab28 60cm retro style left hand hinge fr...   \n",
            "35309     candy 60cm built under larder fridge cru160nek   \n",
            "35310           neff k4316x7gb built under larder fridge   \n",
            "\n",
            "                  Cluster Label  Category Label  \n",
            "0      Apple iPhone 8 Plus 64GB   Mobile Phones  \n",
            "1      Apple iPhone 8 Plus 64GB   Mobile Phones  \n",
            "2      Apple iPhone 8 Plus 64GB   Mobile Phones  \n",
            "3      Apple iPhone 8 Plus 64GB   Mobile Phones  \n",
            "4      Apple iPhone 8 Plus 64GB   Mobile Phones  \n",
            "...                         ...             ...  \n",
            "35306          Smeg FAB28 Cream         Fridges  \n",
            "35307            Smeg FAB28 Red         Fridges  \n",
            "35308           Smeg FAB28 Pink         Fridges  \n",
            "35309             Candy CRU16.0         Fridges  \n",
            "35310                Neff K4316         Fridges  \n",
            "\n",
            "[35311 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# Remove unused columns from data\n",
        "product_df = product_data.drop(product_data.columns[[0, 2, 3, 5]], axis=1)\n",
        "print(product_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring the dataset, there are 7 columns for the data, however I have chosen to only use the 2 necessary columns for my task, these being ‘Product Title’ and ‘Category Label’."
      ],
      "metadata": {
        "id": "Ppvdjy9--6_b"
      },
      "id": "Ppvdjy9--6_b"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ad46de17",
      "metadata": {
        "id": "ad46de17"
      },
      "outputs": [],
      "source": [
        "# Convert the feature columns into numpy array's\n",
        "x = product_df[\"Product Title\"].to_numpy()\n",
        "y = product_df[\" Category Label\"].to_numpy()\n",
        "\n",
        "# Testing outputs\n",
        "#print(x)\n",
        "#print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d9c2d08f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9c2d08f",
        "outputId": "d2d5207b-284d-4c04-e519-c0548fbaa6b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Product Title\n",
            "washing machine                                     90\n",
            "built in fully integrated dishwasher                35\n",
            "frost free fridge freezer                           34\n",
            "washer dryer                                        24\n",
            "american fridge freezer                             22\n",
            "                                                    ..\n",
            "sony hx400v compact camera with 50x optical zoom     1\n",
            "sony hx400 20mp 50x zoom bridge camera               1\n",
            "sony cyber shot hx400 digital camera                 1\n",
            "sony cyber shot dsc hx400v digital camera            1\n",
            "x t3 body only silver                                1\n",
            "Name: count, Length: 30993, dtype: int64\n",
            " Category Label\n",
            "Fridge Freezers     5501\n",
            "Mobile Phones       4081\n",
            "Washing Machines    4044\n",
            "CPUs                3862\n",
            "Fridges             3584\n",
            "TVs                 3564\n",
            "Dishwashers         3424\n",
            "Digital Cameras     2697\n",
            "Microwaves          2342\n",
            "Freezers            2212\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Checking Balance\n",
        "\n",
        "print(product_df[\"Product Title\"].value_counts())\n",
        "print(product_df[\" Category Label\"].value_counts())\n",
        "\n",
        "# Balance is good"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For pre-processing, I chose to use tokenisation and lemmatizer. I initially used stemming but found that it just took away letters from the end of words rather than finding a common base-root. With the lemmatizer, it uses a detailed dictionary of pre-defined words to always make sure a relevant word is returned. I used this for better accuracy and to help the model find only the keywords."
      ],
      "metadata": {
        "id": "mmfAYPiY_Fk_"
      },
      "id": "mmfAYPiY_Fk_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-Processing using tokenisation and lemmatizer\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def prep(X):\n",
        "  prep_text = []\n",
        "  wnl = WordNetLemmatizer() # Initialise Lemmatizer\n",
        "  for x in X:\n",
        "        token_text = word_tokenize(x)\n",
        "        normd_text = [token.lower() for token in token_text if token.isalpha()]\n",
        "\n",
        "        swr_text = [token for token in normd_text if token not in stopwords.words('english')]\n",
        "\n",
        "        prep_text += [[wnl.lemmatize(word, pos=\"v\") for word in swr_text]]\n",
        "\n",
        "  prep_sentences = [\" \".join(sentence) for sentence in prep_text]\n",
        "  return prep_sentences\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxkNcP-3xq0e",
        "outputId": "0acc2305-eae1-4f17-d000-9bea38aa9a9b"
      },
      "id": "kxkNcP-3xq0e",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee4b0061",
      "metadata": {
        "id": "ee4b0061"
      },
      "source": [
        "### Section 2 - Representation Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For my representation learner, I have decided to use TfidfVectorizer from the SKLearn language toolkit. It is commonly used to convert collections of raw data into a matrix of TF-IDF. It starts by tokenizing the text, splitting it into words or n-grams then builds a vocabulary of known words allowing for each word to be given an TF-IDF score and outputs a sparse matrix that can be used by pipelines or models. This works for my dataset as it highlights the important words that are in each product title. For example, for the record ‘apple iphone 8 plus 64gb silver’, it would find the important word phone, as well as weighting down more common words such as ‘plus’ which makes it easier for it to be classified in the category ‘Mobile Phones’."
      ],
      "metadata": {
        "id": "J4n4xXPkaqg3"
      },
      "id": "J4n4xXPkaqg3"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e42e6ead",
      "metadata": {
        "id": "e42e6ead"
      },
      "outputs": [],
      "source": [
        "# Representation Learner - Vectorising\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_base = TfidfVectorizer(max_features=2000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7562a92c",
      "metadata": {
        "id": "7562a92c"
      },
      "source": [
        "### Section 3 - Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algorithm 1 - K-Nearest Neighbour (kNN)\n",
        "\n",
        "kNN is an algorithm used for text classification and works on a similarity based method. A kNN algorithm is designed to predict the label of data points based on the labels of its k closest neighbours. For my model, I build this into a pipeline which also uses my pre-processing function and representation learner. A kNN allows you to decide how many neighbours it should check on each data point and for this I decided on 5 as it gives a nice average for each. The kNN then measures the distance from the current data point to all other points in the training set. After this it sorts the training points by distance and selects the k closest neighbour. Finally, the model predicts the label of each product by taking a majority vote between the k neighbour labels and returns the predicted label for the product. I chose kNN as one of my models because it is able to naturally handle multi-class problems while still being simple and fast and fits the dataset well as most products have similar words in their title to their respective labels.\n"
      ],
      "metadata": {
        "id": "__9mzYxOo2qJ"
      },
      "id": "__9mzYxOo2qJ"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "93c5e230",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93c5e230",
        "outputId": "1a37d81b-09d0-45a9-a67c-0b0e661fb034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8513204519762322\n"
          ]
        }
      ],
      "source": [
        "# Testing with K-Nearest Neighbour in a pipeline\n",
        "\n",
        "# SKLearn Imports\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "knn_acc_score = []\n",
        "\n",
        "# Creates a FunctionTransformer for the prep function\n",
        "prep_transformer = FunctionTransformer(prep, validate=False)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "  ('prep', prep_transformer),\n",
        "  ('rep', TfidfVectorizer()),\n",
        "  ('mod', KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "for train, test in kf.split(x,y):\n",
        "  x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]\n",
        "\n",
        "  pipeline.fit(x_train, y_train)\n",
        "  predictions = pipeline.predict(x_test)\n",
        "  acc = accuracy_score(predictions, y_test)\n",
        "  knn_acc_score.append(acc)\n",
        "\n",
        "print(\"Accuracy:\", np.mean(knn_acc_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algorithm 2 - Multi-Layer Perception (MLP)\n",
        "\n",
        "A MLP is a type of neural algorithm which gets built up by many different neural layers. It is commonly used for text classification tasks like this one. The one that I use is a sequential model which runs each layer in order of how they are coded. I decided on 3 layers, the first one being the input layer. The input layer takes the raw data values, which in my case is the TF-IDF scores from my representation learner. I have then used a hidden layer where a weighted sum is used to perform a ‘relu’ function. Finally, there is the output layer which produces a final prediction using softmax activation. I then use the model for testing by looping through the arrays and running the model on each data point.\n"
      ],
      "metadata": {
        "id": "v-ZybqD4o4TX"
      },
      "id": "v-ZybqD4o4TX"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "519e871e",
      "metadata": {
        "id": "519e871e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0017895-f3e1-4c97-ba4f-610617baa948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.6104 - loss: 1.7754 - val_accuracy: 0.3975 - val_loss: 3.7195\n",
            "Epoch 2/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9644 - loss: 0.1376 - val_accuracy: 0.4078 - val_loss: 4.4351\n",
            "Epoch 3/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9714 - loss: 0.0878 - val_accuracy: 0.4085 - val_loss: 4.8541\n",
            "Epoch 4/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9756 - loss: 0.0699 - val_accuracy: 0.4055 - val_loss: 4.9613\n",
            "Epoch 5/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9774 - loss: 0.0655 - val_accuracy: 0.4103 - val_loss: 5.0989\n",
            "Epoch 6/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9760 - loss: 0.0684 - val_accuracy: 0.4131 - val_loss: 5.2463\n",
            "Epoch 7/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9769 - loss: 0.0637 - val_accuracy: 0.4104 - val_loss: 5.4763\n",
            "Epoch 8/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9775 - loss: 0.0625 - val_accuracy: 0.4133 - val_loss: 5.3338\n",
            "Epoch 9/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9770 - loss: 0.0598 - val_accuracy: 0.4150 - val_loss: 5.5327\n",
            "Epoch 10/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9765 - loss: 0.0588 - val_accuracy: 0.4166 - val_loss: 5.5372\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9745 - loss: 0.1363\n",
            "Test results - Loss: 1.1932132244110107 - Accuracy: 0.859691321849823%\n",
            "Epoch 1/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.5589 - loss: 1.7955 - val_accuracy: 0.3819 - val_loss: 3.6124\n",
            "Epoch 2/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9647 - loss: 0.1406 - val_accuracy: 0.3988 - val_loss: 4.3481\n",
            "Epoch 3/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9716 - loss: 0.0839 - val_accuracy: 0.3958 - val_loss: 4.7372\n",
            "Epoch 4/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9701 - loss: 0.0805 - val_accuracy: 0.4181 - val_loss: 4.9474\n",
            "Epoch 5/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9756 - loss: 0.0643 - val_accuracy: 0.4051 - val_loss: 5.1116\n",
            "Epoch 6/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9749 - loss: 0.0661 - val_accuracy: 0.4172 - val_loss: 5.1655\n",
            "Epoch 7/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9756 - loss: 0.0626 - val_accuracy: 0.3995 - val_loss: 5.3228\n",
            "Epoch 8/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9765 - loss: 0.0615 - val_accuracy: 0.4140 - val_loss: 5.3371\n",
            "Epoch 9/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9753 - loss: 0.0638 - val_accuracy: 0.4126 - val_loss: 5.4765\n",
            "Epoch 10/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9754 - loss: 0.0606 - val_accuracy: 0.4087 - val_loss: 5.5044\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9776 - loss: 0.1197\n",
            "Test results - Loss: 1.1637322902679443 - Accuracy: 0.8609458804130554%\n",
            "Epoch 1/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.6493 - loss: 1.7624 - val_accuracy: 0.3627 - val_loss: 4.0961\n",
            "Epoch 2/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9643 - loss: 0.1404 - val_accuracy: 0.4076 - val_loss: 4.8583\n",
            "Epoch 3/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9744 - loss: 0.0840 - val_accuracy: 0.4000 - val_loss: 5.2983\n",
            "Epoch 4/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9745 - loss: 0.0727 - val_accuracy: 0.4216 - val_loss: 5.4791\n",
            "Epoch 5/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9753 - loss: 0.0687 - val_accuracy: 0.4081 - val_loss: 5.6384\n",
            "Epoch 6/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9756 - loss: 0.0658 - val_accuracy: 0.4177 - val_loss: 5.8129\n",
            "Epoch 7/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9739 - loss: 0.0654 - val_accuracy: 0.4143 - val_loss: 5.8329\n",
            "Epoch 8/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9763 - loss: 0.0621 - val_accuracy: 0.4166 - val_loss: 5.9359\n",
            "Epoch 9/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9773 - loss: 0.0610 - val_accuracy: 0.4170 - val_loss: 5.9744\n",
            "Epoch 10/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9766 - loss: 0.0593 - val_accuracy: 0.4161 - val_loss: 6.0531\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9724 - loss: 0.1367\n",
            "Test results - Loss: 1.2705178260803223 - Accuracy: 0.8572642207145691%\n",
            "Epoch 1/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.6338 - loss: 1.7815 - val_accuracy: 0.3646 - val_loss: 3.7297\n",
            "Epoch 2/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9647 - loss: 0.1380 - val_accuracy: 0.3965 - val_loss: 4.5399\n",
            "Epoch 3/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9742 - loss: 0.0820 - val_accuracy: 0.4097 - val_loss: 4.7991\n",
            "Epoch 4/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9743 - loss: 0.0740 - val_accuracy: 0.4181 - val_loss: 5.1174\n",
            "Epoch 5/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9757 - loss: 0.0684 - val_accuracy: 0.4135 - val_loss: 5.1960\n",
            "Epoch 6/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9742 - loss: 0.0684 - val_accuracy: 0.4120 - val_loss: 5.3568\n",
            "Epoch 7/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9763 - loss: 0.0637 - val_accuracy: 0.4094 - val_loss: 5.4481\n",
            "Epoch 8/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9745 - loss: 0.0669 - val_accuracy: 0.4161 - val_loss: 5.6142\n",
            "Epoch 9/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9769 - loss: 0.0616 - val_accuracy: 0.4103 - val_loss: 5.6016\n",
            "Epoch 10/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9759 - loss: 0.0649 - val_accuracy: 0.4122 - val_loss: 5.7511\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9756 - loss: 0.1291\n",
            "Test results - Loss: 1.2163443565368652 - Accuracy: 0.8606626987457275%\n",
            "Epoch 1/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.6270 - loss: 1.7722 - val_accuracy: 0.3696 - val_loss: 3.6251\n",
            "Epoch 2/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9640 - loss: 0.1390 - val_accuracy: 0.3786 - val_loss: 4.4332\n",
            "Epoch 3/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9708 - loss: 0.0878 - val_accuracy: 0.4115 - val_loss: 4.6615\n",
            "Epoch 4/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9743 - loss: 0.0762 - val_accuracy: 0.4101 - val_loss: 4.9203\n",
            "Epoch 5/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9775 - loss: 0.0660 - val_accuracy: 0.3993 - val_loss: 5.1315\n",
            "Epoch 6/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.0711 - val_accuracy: 0.4159 - val_loss: 5.2440\n",
            "Epoch 7/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9737 - loss: 0.0667 - val_accuracy: 0.4156 - val_loss: 5.3300\n",
            "Epoch 8/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9745 - loss: 0.0660 - val_accuracy: 0.4099 - val_loss: 5.3335\n",
            "Epoch 9/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9749 - loss: 0.0641 - val_accuracy: 0.4112 - val_loss: 5.4622\n",
            "Epoch 10/10\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9742 - loss: 0.0616 - val_accuracy: 0.4081 - val_loss: 5.6606\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9713 - loss: 0.1298\n",
            "Test results - Loss: 1.2247384786605835 - Accuracy: 0.8541489839553833%\n",
            "Accuracy: 0.8586984634399414\n"
          ]
        }
      ],
      "source": [
        "# Testing with Multi Layer Perception to compare accuracy scores\n",
        "# Tensorflow imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# SKLearn Imports\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "def mlp(input_dimension, num_classes):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, input_shape=(input_dimension,), activation='relu'))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "xnp = np.array(x) #convert to numpy to standardise the arrays for the split\n",
        "ynp = np.array(y)\n",
        "\n",
        "# Initialize mlp_acc_score list if it's not already\n",
        "if 'mlp_acc_score' not in locals():\n",
        "    mlp_acc_score = []\n",
        "\n",
        "for train, test in kf.split(xnp,ynp):\n",
        "  x_train, x_test, y_train, y_test = xnp[train], xnp[test], ynp[train], ynp[test]\n",
        "\n",
        "  x_train = prep(x_train) #we preprocess our train and test datasets\n",
        "  x_test = prep(x_test)\n",
        "\n",
        "  # Instantiate TfidfVectorizer for each fold to prevent data leakage\n",
        "  tfidf_vectorizer = tfidf_base\n",
        "  x_train = tfidf_vectorizer.fit_transform(x_train)\n",
        "  x_train = x_train.todense() #by default, tfidf will output a sparse matris to conserve memory. This is incompatible with our deep learner\n",
        "  x_test = tfidf_vectorizer.transform(x_test)\n",
        "  x_test = x_test.todense()\n",
        "\n",
        "  # Encode string labels to integers before one-hot encoding\n",
        "  y_train_encoded = np.array([label_to_int[label] for label in y_train])\n",
        "  y_test_encoded = np.array([label_to_int[label] for label in y_test])\n",
        "\n",
        "  model = mlp(TFIDF_MAX_FEATURES, NUM_CLASSES)\n",
        "  y_train_one_hot = to_categorical(y_train_encoded, NUM_CLASSES)\n",
        "  y_test_one_hot = to_categorical(y_test_encoded, NUM_CLASSES)\n",
        "\n",
        "  # Configure the model and start training\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.fit(x_train, y_train_one_hot, epochs=10, batch_size=250, verbose=1, validation_split=0.2)\n",
        "\n",
        "  # Test the model after training\n",
        "  test_results = model.evaluate(x_test, y_test_one_hot, verbose=1)\n",
        "  print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')\n",
        "\n",
        "  mlp_acc_score.append(test_results[1])\n",
        "\n",
        "print(\"Accuracy:\", np.mean(mlp_acc_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd7583e0",
      "metadata": {
        "id": "cd7583e0"
      },
      "source": [
        "### Section 4 - Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at each algorithm’s performance, the kNN algorithm produced an accuracy score of 0.851, however had a wider range of scores from 0.80 to 0.88, making it a slightly inconsistent model. However, the MLP model had a very similar average score with roughly 0.859, but was a more consistent model giving similar scores for each run."
      ],
      "metadata": {
        "id": "ar5buETxo-CU"
      },
      "id": "ar5buETxo-CU"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6683c333",
      "metadata": {
        "id": "6683c333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "d4178a1e-3173-415e-e689-cfb4d0164cc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(1, 0, 'knn'), Text(2, 0, 'mlp')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib as plt\n",
        "\n",
        "model_comp = []\n",
        "model_comp.append(knn_acc_score)\n",
        "model_comp.append(mlp_acc_score)\n",
        "\n",
        "fig, ax2 = plt.pyplot.subplots()\n",
        "ax2.set_title('Algorithm Performance')\n",
        "ax2.boxplot(model_comp)\n",
        "ax2.set_xticklabels(['knn', 'mlp'], rotation='vertical')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at this, the MLP is the better algorithm for consistency, but the kNN has the capability to produce higher accuracy scores but with more variability."
      ],
      "metadata": {
        "id": "WGP8f2ephu7b"
      },
      "id": "WGP8f2ephu7b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}